{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ik3yHkxcmh6s"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
    "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
    "import kagglehub\n",
    "kagglehub.login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCzCVmi0mh66"
   },
   "outputs": [],
   "source": [
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "data_255_toxic_comment_in_class_competition_path = kagglehub.competition_download('data-255-toxic-comment-in-class-competition')\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJjXgmcWmh67",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:41:11.173008Z",
     "iopub.status.busy": "2024-12-03T19:41:11.172732Z",
     "iopub.status.idle": "2024-12-03T19:41:16.509604Z",
     "shell.execute_reply": "2024-12-03T19:41:16.508933Z",
     "shell.execute_reply.started": "2024-12-03T19:41:11.172976Z"
    },
    "id": "VXtGU7clmh67",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "# Inspect the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:41:16.511942Z",
     "iopub.status.busy": "2024-12-03T19:41:16.511193Z",
     "iopub.status.idle": "2024-12-03T19:41:32.701163Z",
     "shell.execute_reply": "2024-12-03T19:41:32.700257Z",
     "shell.execute_reply.started": "2024-12-03T19:41:16.511901Z"
    },
    "id": "kBlIpcJEmh69",
    "outputId": "2aabcbdf-99be-4a88-cfda-586bc19e1498",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  toxicity  \\\n",
      "0   0  This is so cool. It's like, 'would you want yo...  0.000000   \n",
      "1   1  Thank you!! This would make my life a lot less...  0.000000   \n",
      "2   2  This is such an urgent design problem; kudos t...  0.000000   \n",
      "3   3  Is this something I'll be able to install on m...  0.000000   \n",
      "4   4               haha you guys are a bunch of losers.  0.893617   \n",
      "\n",
      "   severe_toxicity  obscene  threat   insult  identity_attack  sexual_explicit  \n",
      "0         0.000000      0.0     0.0  0.00000         0.000000              0.0  \n",
      "1         0.000000      0.0     0.0  0.00000         0.000000              0.0  \n",
      "2         0.000000      0.0     0.0  0.00000         0.000000              0.0  \n",
      "3         0.000000      0.0     0.0  0.00000         0.000000              0.0  \n",
      "4         0.021277      0.0     0.0  0.87234         0.021277              0.0  \n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('/kaggle/input/data-255-toxic-comment-in-class-competition/train.csv')\n",
    "\n",
    "print(data.head())\n",
    "data['text'] = data['text'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:41:32.702401Z",
     "iopub.status.busy": "2024-12-03T19:41:32.702157Z",
     "iopub.status.idle": "2024-12-03T19:42:33.406796Z",
     "shell.execute_reply": "2024-12-03T19:42:33.406102Z",
     "shell.execute_reply.started": "2024-12-03T19:41:32.702377Z"
    },
    "id": "Kk1AvuSomh6-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = ''\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text,flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra whitespace\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "data['clean_text'] = data['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:42:33.408671Z",
     "iopub.status.busy": "2024-12-03T19:42:33.408398Z",
     "iopub.status.idle": "2024-12-03T19:42:33.423161Z",
     "shell.execute_reply": "2024-12-03T19:42:33.422261Z",
     "shell.execute_reply.started": "2024-12-03T19:42:33.408645Z"
    },
    "id": "HusV7kRBmh6-",
    "outputId": "14ff6d0f-d974-40a5-a613-3407f220fa5a",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this is so cool its like would you want your m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thank you this would make my life a lot less a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>this is such an urgent design problem kudos to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>is this something ill be able to install on my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>haha you guys are a bunch of losers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  toxicity  \\\n",
       "0   0  This is so cool. It's like, 'would you want yo...  0.000000   \n",
       "1   1  Thank you!! This would make my life a lot less...  0.000000   \n",
       "2   2  This is such an urgent design problem; kudos t...  0.000000   \n",
       "3   3  Is this something I'll be able to install on m...  0.000000   \n",
       "4   4               haha you guys are a bunch of losers.  0.893617   \n",
       "\n",
       "   severe_toxicity  obscene  threat   insult  identity_attack  \\\n",
       "0         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "1         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "2         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "3         0.000000      0.0     0.0  0.00000         0.000000   \n",
       "4         0.021277      0.0     0.0  0.87234         0.021277   \n",
       "\n",
       "   sexual_explicit                                         clean_text  \n",
       "0              0.0  this is so cool its like would you want your m...  \n",
       "1              0.0  thank you this would make my life a lot less a...  \n",
       "2              0.0  this is such an urgent design problem kudos to...  \n",
       "3              0.0  is this something ill be able to install on my...  \n",
       "4              0.0                haha you guys are a bunch of losers  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:42:33.425024Z",
     "iopub.status.busy": "2024-12-03T19:42:33.424288Z",
     "iopub.status.idle": "2024-12-03T19:43:08.814723Z",
     "shell.execute_reply": "2024-12-03T19:43:08.813746Z",
     "shell.execute_reply.started": "2024-12-03T19:42:33.424986Z"
    },
    "id": "mCmWCwmsmh7C",
    "outputId": "b0c36f52-905d-459d-9040-6a3de4d42c3a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2598 empty samples.\n",
      "573750 vocab size\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Tokenize all texts\n",
    "data['tokens'] = data['clean_text'].apply(tokenize)\n",
    "\n",
    "initial_count = len(data)\n",
    "data = data[data['tokens'].apply(len) > 0].reset_index(drop=True)\n",
    "filtered_count = len(data)\n",
    "print(f'Removed {initial_count - filtered_count} empty samples.')\n",
    "\n",
    "# Proceed only if there are samples left\n",
    "if filtered_count == 0:\n",
    "    raise ValueError(\"All samples have empty sequences after preprocessing.\")\n",
    "\n",
    "\n",
    "# Build vocabulary\n",
    "all_tokens = [token for tokens in data['tokens'] for token in tokens]\n",
    "counter = Counter(all_tokens)\n",
    "vocab = sorted(counter, key=counter.get, reverse=True)\n",
    "\n",
    "# Create word to index mapping\n",
    "vocab_size = len(vocab) + 2  # +2 for PAD and UNK\n",
    "print(vocab_size,\"vocab size\")\n",
    "word2idx = {word: idx+2 for idx, word in enumerate(vocab)}\n",
    "word2idx['<PAD>'] = 0\n",
    "word2idx['<UNK>'] = 1\n",
    "\n",
    "# Save the vocabulary for future use\n",
    "with open('word2idx.pkl', 'wb') as f:\n",
    "    pickle.dump(word2idx, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:08.816611Z",
     "iopub.status.busy": "2024-12-03T19:43:08.815961Z",
     "iopub.status.idle": "2024-12-03T19:43:35.30073Z",
     "shell.execute_reply": "2024-12-03T19:43:35.30002Z",
     "shell.execute_reply.started": "2024-12-03T19:43:08.816577Z"
    },
    "id": "l1b7HWaLmh7D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokens_to_indices(tokens, word2idx):\n",
    "    return [word2idx.get(token, word2idx['<UNK>']) for token in tokens]\n",
    "\n",
    "data['sequence'] = data['tokens'].apply(lambda x: tokens_to_indices(x, word2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:35.301999Z",
     "iopub.status.busy": "2024-12-03T19:43:35.30171Z",
     "iopub.status.idle": "2024-12-03T19:43:45.651206Z",
     "shell.execute_reply": "2024-12-03T19:43:45.650215Z",
     "shell.execute_reply.started": "2024-12-03T19:43:35.301972Z"
    },
    "id": "wdDB6si0mh7D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "label_cols = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "data['labels'] = data[label_cols].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:45.652541Z",
     "iopub.status.busy": "2024-12-03T19:43:45.652261Z",
     "iopub.status.idle": "2024-12-03T19:43:47.661728Z",
     "shell.execute_reply": "2024-12-03T19:43:47.660741Z",
     "shell.execute_reply.started": "2024-12-03T19:43:45.652514Z"
    },
    "id": "UrYiOyzHmh7D",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify no empty sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:47.663736Z",
     "iopub.status.busy": "2024-12-03T19:43:47.66325Z",
     "iopub.status.idle": "2024-12-03T19:43:48.442922Z",
     "shell.execute_reply": "2024-12-03T19:43:48.441947Z",
     "shell.execute_reply.started": "2024-12-03T19:43:47.663694Z"
    },
    "id": "VPM3-Dkvmh7D",
    "outputId": "91e4ccb5-e5fd-4f18-a271-1cfca0a0d308",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty sequences in train set: 0\n",
      "Empty sequences in validation set: 0\n"
     ]
    }
   ],
   "source": [
    "empty_sequences = train_data['tokens'].apply(len) == 0\n",
    "print(f'Empty sequences in train set: {empty_sequences.sum()}')\n",
    "\n",
    "empty_sequences = val_data['tokens'].apply(len) == 0\n",
    "print(f'Empty sequences in validation set: {empty_sequences.sum()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:48.446008Z",
     "iopub.status.busy": "2024-12-03T19:43:48.445721Z",
     "iopub.status.idle": "2024-12-03T19:43:48.453837Z",
     "shell.execute_reply": "2024-12-03T19:43:48.452824Z",
     "shell.execute_reply.started": "2024-12-03T19:43:48.445981Z"
    },
    "id": "hhqzz3hamh7E",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, data, word2idx, max_len=100):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data.iloc[idx]['sequence']\n",
    "        label = self.data.iloc[idx]['labels']\n",
    "        # Truncate or pad the sequence\n",
    "        if len(seq) > self.max_len:\n",
    "            seq = seq[:self.max_len]\n",
    "            length = self.max_len\n",
    "        else:\n",
    "            length = len(seq)\n",
    "            seq = seq + [self.word2idx['<PAD>']] * (self.max_len - len(seq))\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(label, dtype=torch.float32), torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "batch_size = 256\n",
    "max_len = 128\n",
    "\n",
    "train_dataset = CommentDataset(train_data, word2idx, max_len)\n",
    "val_dataset = CommentDataset(val_data, word2idx, max_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:48.455239Z",
     "iopub.status.busy": "2024-12-03T19:43:48.454847Z",
     "iopub.status.idle": "2024-12-03T19:43:48.470791Z",
     "shell.execute_reply": "2024-12-03T19:43:48.469948Z",
     "shell.execute_reply.started": "2024-12-03T19:43:48.455196Z"
    },
    "id": "cEbhehBFmh7E",
    "outputId": "dcf6863f-6c7f-4e2a-9780-5d0b84e2e55c",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7c4906fde8c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:48.47212Z",
     "iopub.status.busy": "2024-12-03T19:43:48.471754Z",
     "iopub.status.idle": "2024-12-03T19:43:48.484016Z",
     "shell.execute_reply": "2024-12-03T19:43:48.483234Z",
     "shell.execute_reply.started": "2024-12-03T19:43:48.472079Z"
    },
    "id": "Ds3STq7Pmh7E",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, 128)\n",
    "        self.fc_out = nn.Linear(128, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, lengths):\n",
    "        # Ensure lengths are on CPU and of type torch.long\n",
    "        lengths = lengths.cpu()\n",
    "        # text: [batch size, seq len]\n",
    "        embedded = self.embedding(text)  # [batch size, seq len, embedding dim]\n",
    "        # Pack the sequences\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        output = self.fc(hidden)\n",
    "        return self.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:48.485005Z",
     "iopub.status.busy": "2024-12-03T19:43:48.484741Z",
     "iopub.status.idle": "2024-12-03T19:43:50.399516Z",
     "shell.execute_reply": "2024-12-03T19:43:50.398815Z",
     "shell.execute_reply.started": "2024-12-03T19:43:48.484979Z"
    },
    "id": "3Sw9zk_nmh7E",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = len(label_cols)  # 7\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.3\n",
    "pad_idx = word2idx['<PAD>']\n",
    "\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(train_data['labels']), y=train_data['labels'])\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = nn.BCELoss(weight=class_weights_tensor)\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = criterion.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define train and evaluate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:50.400789Z",
     "iopub.status.busy": "2024-12-03T19:43:50.400469Z",
     "iopub.status.idle": "2024-12-03T19:43:50.407678Z",
     "shell.execute_reply": "2024-12-03T19:43:50.406929Z",
     "shell.execute_reply.started": "2024-12-03T19:43:50.400765Z"
    },
    "id": "RrHCTfmAmh7E",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for sequences, labels, lengths in dataloader:\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(sequences, lengths)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels, lengths in dataloader:\n",
    "            sequences = sequences.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Do NOT move lengths to device\n",
    "            # lengths = lengths.to(device)\n",
    "\n",
    "            predictions = model(sequences, lengths)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    # Binarize the predictions and labels\n",
    "    all_preds_binary = (all_preds >= 0.5).astype(int)\n",
    "    all_labels_binary = (all_labels >= 0.5).astype(int)\n",
    "\n",
    "    return epoch_loss / len(dataloader), all_preds_binary, all_labels_binary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-03T19:43:50.409083Z",
     "iopub.status.busy": "2024-12-03T19:43:50.408746Z",
     "iopub.status.idle": "2024-12-04T04:05:34.81656Z",
     "shell.execute_reply": "2024-12-04T04:05:34.815652Z",
     "shell.execute_reply.started": "2024-12-03T19:43:50.409046Z"
    },
    "id": "hExDFl_1mh7F",
    "outputId": "f9a7987e-d271-4088-fa11-c401a0b41c65",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 0.1032\n",
      "Val Loss: 0.0950 | Val F1: 0.3195 | Val Accuracy: 0.9396\n",
      "Model saved!\n",
      "Epoch 2/50\n",
      "Train Loss: 0.0932\n",
      "Val Loss: 0.0934 | Val F1: 0.3556 | Val Accuracy: 0.9400\n",
      "Model saved!\n",
      "Epoch 3/50\n",
      "Train Loss: 0.0911\n",
      "Val Loss: 0.0931 | Val F1: 0.3484 | Val Accuracy: 0.9406\n",
      "Model saved!\n",
      "Epoch 4/50\n",
      "Train Loss: 0.0893\n",
      "Val Loss: 0.0933 | Val F1: 0.3751 | Val Accuracy: 0.9413\n",
      "Epoch 5/50\n",
      "Train Loss: 0.0876\n",
      "Val Loss: 0.0941 | Val F1: 0.3782 | Val Accuracy: 0.9397\n",
      "Epoch 6/50\n",
      "Train Loss: 0.0858\n",
      "Val Loss: 0.0949 | Val F1: 0.3885 | Val Accuracy: 0.9399\n",
      "Epoch 7/50\n",
      "Train Loss: 0.0840\n",
      "Val Loss: 0.0961 | Val F1: 0.4017 | Val Accuracy: 0.9390\n",
      "Epoch 8/50\n",
      "Train Loss: 0.0823\n",
      "Val Loss: 0.0973 | Val F1: 0.3866 | Val Accuracy: 0.9388\n",
      "Epoch 9/50\n",
      "Train Loss: 0.0807\n",
      "Val Loss: 0.0986 | Val F1: 0.4013 | Val Accuracy: 0.9379\n",
      "Epoch 10/50\n",
      "Train Loss: 0.0792\n",
      "Val Loss: 0.1008 | Val F1: 0.4011 | Val Accuracy: 0.9368\n",
      "Epoch 11/50\n",
      "Train Loss: 0.0780\n",
      "Val Loss: 0.1024 | Val F1: 0.4044 | Val Accuracy: 0.9358\n",
      "Epoch 12/50\n",
      "Train Loss: 0.0769\n",
      "Val Loss: 0.1040 | Val F1: 0.4123 | Val Accuracy: 0.9351\n",
      "Epoch 13/50\n",
      "Train Loss: 0.0760\n",
      "Val Loss: 0.1056 | Val F1: 0.3973 | Val Accuracy: 0.9363\n",
      "Epoch 14/50\n",
      "Train Loss: 0.0753\n",
      "Val Loss: 0.1075 | Val F1: 0.3977 | Val Accuracy: 0.9361\n",
      "Epoch 15/50\n",
      "Train Loss: 0.0746\n",
      "Val Loss: 0.1104 | Val F1: 0.3980 | Val Accuracy: 0.9342\n",
      "Epoch 16/50\n",
      "Train Loss: 0.0741\n",
      "Val Loss: 0.1101 | Val F1: 0.4006 | Val Accuracy: 0.9356\n",
      "Epoch 17/50\n",
      "Train Loss: 0.0736\n",
      "Val Loss: 0.1126 | Val F1: 0.4018 | Val Accuracy: 0.9352\n",
      "Epoch 18/50\n",
      "Train Loss: 0.0732\n",
      "Val Loss: 0.1123 | Val F1: 0.3926 | Val Accuracy: 0.9355\n",
      "Epoch 19/50\n",
      "Train Loss: 0.0729\n",
      "Val Loss: 0.1142 | Val F1: 0.3906 | Val Accuracy: 0.9346\n",
      "Epoch 20/50\n",
      "Train Loss: 0.0726\n",
      "Val Loss: 0.1143 | Val F1: 0.3981 | Val Accuracy: 0.9338\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0723\n",
      "Val Loss: 0.1124 | Val F1: 0.4135 | Val Accuracy: 0.9332\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0721\n",
      "Val Loss: 0.1156 | Val F1: 0.3878 | Val Accuracy: 0.9349\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0719\n",
      "Val Loss: 0.1159 | Val F1: 0.4009 | Val Accuracy: 0.9330\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0717\n",
      "Val Loss: 0.1161 | Val F1: 0.3958 | Val Accuracy: 0.9331\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0715\n",
      "Val Loss: 0.1167 | Val F1: 0.3832 | Val Accuracy: 0.9340\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0713\n",
      "Val Loss: 0.1181 | Val F1: 0.4017 | Val Accuracy: 0.9326\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0712\n",
      "Val Loss: 0.1189 | Val F1: 0.3992 | Val Accuracy: 0.9336\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0710\n",
      "Val Loss: 0.1172 | Val F1: 0.4000 | Val Accuracy: 0.9323\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0709\n",
      "Val Loss: 0.1178 | Val F1: 0.3926 | Val Accuracy: 0.9334\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0708\n",
      "Val Loss: 0.1174 | Val F1: 0.4013 | Val Accuracy: 0.9317\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0707\n",
      "Val Loss: 0.1182 | Val F1: 0.3955 | Val Accuracy: 0.9313\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0705\n",
      "Val Loss: 0.1194 | Val F1: 0.3888 | Val Accuracy: 0.9320\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0704\n",
      "Val Loss: 0.1198 | Val F1: 0.3910 | Val Accuracy: 0.9318\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0703\n",
      "Val Loss: 0.1215 | Val F1: 0.3941 | Val Accuracy: 0.9335\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0702\n",
      "Val Loss: 0.1209 | Val F1: 0.3911 | Val Accuracy: 0.9326\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0701\n",
      "Val Loss: 0.1213 | Val F1: 0.3985 | Val Accuracy: 0.9317\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0701\n",
      "Val Loss: 0.1204 | Val F1: 0.3909 | Val Accuracy: 0.9339\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0700\n",
      "Val Loss: 0.1207 | Val F1: 0.3898 | Val Accuracy: 0.9317\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0699\n",
      "Val Loss: 0.1222 | Val F1: 0.3882 | Val Accuracy: 0.9318\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0699\n",
      "Val Loss: 0.1209 | Val F1: 0.3863 | Val Accuracy: 0.9322\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0697\n",
      "Val Loss: 0.1223 | Val F1: 0.3958 | Val Accuracy: 0.9304\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0697\n",
      "Val Loss: 0.1211 | Val F1: 0.3946 | Val Accuracy: 0.9318\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0696\n",
      "Val Loss: 0.1217 | Val F1: 0.3835 | Val Accuracy: 0.9324\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0696\n",
      "Val Loss: 0.1225 | Val F1: 0.3899 | Val Accuracy: 0.9322\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0695\n",
      "Val Loss: 0.1219 | Val F1: 0.3882 | Val Accuracy: 0.9318\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0694\n",
      "Val Loss: 0.1230 | Val F1: 0.3858 | Val Accuracy: 0.9324\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0694\n",
      "Val Loss: 0.1234 | Val F1: 0.3920 | Val Accuracy: 0.9315\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0694\n",
      "Val Loss: 0.1249 | Val F1: 0.3862 | Val Accuracy: 0.9290\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0693\n",
      "Val Loss: 0.1228 | Val F1: 0.3950 | Val Accuracy: 0.9310\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0693\n",
      "Val Loss: 0.1236 | Val F1: 0.3955 | Val Accuracy: 0.9316\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_preds_binary, val_labels_binary = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Calculate F1 Score and Accuracy\n",
    "    f1 = f1_score(val_labels_binary, val_preds_binary, average='macro')\n",
    "    accuracy = accuracy_score(val_labels_binary, val_preds_binary)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {train_loss:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val F1: {f1:.4f} | Val Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss decreases\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        print('Model saved!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the best model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:21:04.897248Z",
     "iopub.status.busy": "2024-12-04T05:21:04.896899Z",
     "iopub.status.idle": "2024-12-04T05:22:18.168158Z",
     "shell.execute_reply": "2024-12-04T05:22:18.167204Z",
     "shell.execute_reply.started": "2024-12-04T05:21:04.897217Z"
    },
    "id": "Q6XelhvHmh7F",
    "outputId": "1c27f39a-cee7-4de8-8603-4a70041601c5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/4026743553.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_mode.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Val Loss: 0.0935\n",
      "Final Val F1 Score: 0.3602\n",
      "Final Val Accuracy: 0.9402\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "val_loss, val_preds_binary, val_labels_binary = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "# Calculate final metrics\n",
    "f1 = f1_score(val_labels_binary, val_preds_binary, average='macro')\n",
    "accuracy = accuracy_score(val_labels_binary, val_preds_binary)\n",
    "\n",
    "print(f'Final Val Loss: {val_loss:.4f}')\n",
    "print(f'Final Val F1 Score: {f1:.4f}')\n",
    "print(f'Final Val Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train best model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T04:19:56.020202Z",
     "iopub.status.busy": "2024-12-04T04:19:56.019814Z",
     "iopub.status.idle": "2024-12-04T05:10:21.241897Z",
     "shell.execute_reply": "2024-12-04T05:10:21.24093Z",
     "shell.execute_reply.started": "2024-12-04T04:19:56.020171Z"
    },
    "id": "-wQDwjnUmh7F",
    "outputId": "fbd6ab3c-d21f-45c0-a383-d05ac6eba9a4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Train Loss: 0.0890\n",
      "Val Loss: 0.0935 | Val F1: 0.3602 | Val Accuracy: 0.9402\n",
      "Model saved!\n",
      "Epoch 2/5\n",
      "Train Loss: 0.0878\n",
      "Val Loss: 0.0940 | Val F1: 0.3943 | Val Accuracy: 0.9394\n",
      "Epoch 3/5\n",
      "Train Loss: 0.0861\n",
      "Val Loss: 0.0949 | Val F1: 0.3824 | Val Accuracy: 0.9404\n",
      "Epoch 4/5\n",
      "Train Loss: 0.0844\n",
      "Val Loss: 0.0957 | Val F1: 0.3889 | Val Accuracy: 0.9394\n",
      "Epoch 5/5\n",
      "Train Loss: 0.0827\n",
      "Val Loss: 0.0981 | Val F1: 0.4050 | Val Accuracy: 0.9382\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_preds_binary, val_labels_binary = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Calculate F1 Score and Accuracy\n",
    "    f1 = f1_score(val_labels_binary, val_preds_binary, average='macro')\n",
    "    accuracy = accuracy_score(val_labels_binary, val_preds_binary)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print(f'Train Loss: {train_loss:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} | Val F1: {f1:.4f} | Val Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Save the model if validation loss decreases\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_mode.pt')\n",
    "        print('Model saved!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Load test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:22:18.170638Z",
     "iopub.status.busy": "2024-12-04T05:22:18.169988Z",
     "iopub.status.idle": "2024-12-04T05:22:28.530458Z",
     "shell.execute_reply": "2024-12-04T05:22:28.52952Z",
     "shell.execute_reply.started": "2024-12-04T05:22:18.170597Z"
    },
    "id": "HYKn3Cmemh7I",
    "outputId": "5448230d-02ea-4558-9617-77f74a295e2f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 137 empty test samples.\n"
     ]
    }
   ],
   "source": [
    "test_data = pd.read_csv('/kaggle/input/data-255-toxic-comment-in-class-competition/test.csv')\n",
    "\n",
    "# Clean text\n",
    "test_data['clean_text'] = test_data['text'].apply(clean_text)\n",
    "\n",
    "# Tokenize\n",
    "test_data['tokens'] = test_data['clean_text'].apply(tokenize)\n",
    "\n",
    "# Remove empty sequences\n",
    "initial_test_count = len(test_data)\n",
    "test_data = test_data[test_data['tokens'].apply(len) > 0].reset_index(drop=True)\n",
    "filtered_test_count = len(test_data)\n",
    "print(f'Removed {initial_test_count - filtered_test_count} empty test samples.')\n",
    "\n",
    "if filtered_test_count == 0:\n",
    "    raise ValueError(\"All test samples have empty sequences after preprocessing.\")\n",
    "\n",
    "# Convert tokens to indices\n",
    "test_data['sequence'] = test_data['tokens'].apply(lambda x: tokens_to_indices(x, word2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:22:28.532383Z",
     "iopub.status.busy": "2024-12-04T05:22:28.531683Z",
     "iopub.status.idle": "2024-12-04T05:22:28.693386Z",
     "shell.execute_reply": "2024-12-04T05:22:28.692413Z",
     "shell.execute_reply.started": "2024-12-04T05:22:28.532341Z"
    },
    "id": "yjB6O5Xmmh7K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data, word2idx, max_len=100):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.data.iloc[idx]['sequence']\n",
    "        # Truncate or pad the sequence\n",
    "        if len(seq) > self.max_len:\n",
    "            seq = seq[:self.max_len]\n",
    "            length = self.max_len\n",
    "        else:\n",
    "            length = len(seq)\n",
    "            seq = seq + [self.word2idx['<PAD>']] * (self.max_len - len(seq))\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(length, dtype=torch.long)\n",
    "\n",
    "test_dataset = TestDataset(test_data, word2idx, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:22:28.697012Z",
     "iopub.status.busy": "2024-12-04T05:22:28.696693Z",
     "iopub.status.idle": "2024-12-04T05:22:41.672738Z",
     "shell.execute_reply": "2024-12-04T05:22:41.67181Z",
     "shell.execute_reply.started": "2024-12-04T05:22:28.696985Z"
    },
    "id": "RdOBIwmSmh7K",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, lengths in dataloader:\n",
    "            sequences = sequences.to(device)\n",
    "            predictions = model(sequences, lengths)\n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    return all_preds\n",
    "\n",
    "test_preds = predict(model, test_loader, device)\n",
    "\n",
    "# Convert probabilities to binary (0 or 1)\n",
    "test_preds_binary = (test_preds >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-04T05:22:41.674265Z",
     "iopub.status.busy": "2024-12-04T05:22:41.673919Z",
     "iopub.status.idle": "2024-12-04T05:22:42.250565Z",
     "shell.execute_reply": "2024-12-04T05:22:42.249698Z",
     "shell.execute_reply.started": "2024-12-04T05:22:41.674226Z"
    },
    "id": "R6XC0glFmh7K",
    "outputId": "f75f4e7a-891d-4fac-c481-d0879b97ecee",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "original_test_ids = pd.read_csv('/kaggle/input/data-255-toxic-comment-in-class-competition/test.csv')['id']\n",
    "\n",
    "filtered_test_ids = test_data['id']\n",
    "\n",
    "removed_test_ids = set(original_test_ids) - set(filtered_test_ids)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': filtered_test_ids,\n",
    "    'toxicity': test_preds_binary[:,0],\n",
    "    'severe_toxicity': test_preds_binary[:,1],\n",
    "    'obscene': test_preds_binary[:,2],\n",
    "    'threat': test_preds_binary[:,3],\n",
    "    'insult': test_preds_binary[:,4],\n",
    "    'identity_attack': test_preds_binary[:,5],\n",
    "    'sexual_explicit': test_preds_binary[:,6],\n",
    "})\n",
    "\n",
    "# For removed test samples, assign 0 to all labels or handle as needed\n",
    "if removed_test_ids:\n",
    "    removed_df = pd.DataFrame({\n",
    "        'id': list(removed_test_ids),\n",
    "        'toxicity': 0,\n",
    "        'severe_toxicity': 0,\n",
    "        'obscene': 0,\n",
    "        'threat': 0,\n",
    "        'insult': 0,\n",
    "        'identity_attack': 0,\n",
    "        'sexual_explicit': 0,\n",
    "    })\n",
    "    # Concatenate predictions with removed samples\n",
    "    submission = pd.concat([predictions_df, removed_df], ignore_index=True)\n",
    "else:\n",
    "    submission = predictions_df\n",
    "\n",
    "# Ensure the submission has the same order as the original test set\n",
    "submission = submission.set_index('id').reindex(original_test_ids).reset_index()\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('predictions.csv', index=False)\n",
    "\n",
    "print('Predictions saved to predictions.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "dl_lab2",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9910482,
     "sourceId": 87217,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
