{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-26T08:57:39.739831Z","iopub.status.busy":"2024-10-26T08:57:39.739114Z","iopub.status.idle":"2024-10-26T08:57:40.692914Z","shell.execute_reply":"2024-10-26T08:57:40.691955Z","shell.execute_reply.started":"2024-10-26T08:57:39.739793Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","# for dirname, _, filenames in os.walk('/kaggle/input'):\n","#     for filename in filenames:\n","#         print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:57:40.695143Z","iopub.status.busy":"2024-10-26T08:57:40.694747Z","iopub.status.idle":"2024-10-26T08:57:45.159711Z","shell.execute_reply":"2024-10-26T08:57:45.158846Z","shell.execute_reply.started":"2024-10-26T08:57:40.695110Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision\n","from torchvision.models.detection import fasterrcnn_resnet50_fpn\n","from torchvision.transforms import functional as F\n","from PIL import Image\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:57:45.161249Z","iopub.status.busy":"2024-10-26T08:57:45.160838Z","iopub.status.idle":"2024-10-26T08:57:45.168288Z","shell.execute_reply":"2024-10-26T08:57:45.167360Z","shell.execute_reply.started":"2024-10-26T08:57:45.161217Z"},"trusted":true},"outputs":[],"source":["import torchvision.transforms as T\n","import random\n","\n","# Define transformations for training\n","train_transforms = T.Compose([\n","    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    T.RandomHorizontalFlip(0.5),  # 50% chance to flip\n","    T.RandomRotation(degrees=10),  # Rotate randomly between -10 to 10 degrees\n","    T.ToTensor(),\n","    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Define transformations for validation (no augmentation, just normalization)\n","valid_transforms = T.Compose([\n","    T.ToTensor(),\n","    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:57:45.170988Z","iopub.status.busy":"2024-10-26T08:57:45.170661Z","iopub.status.idle":"2024-10-26T08:57:45.186261Z","shell.execute_reply":"2024-10-26T08:57:45.185358Z","shell.execute_reply.started":"2024-10-26T08:57:45.170956Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","from torch.utils.data import Dataset\n","from PIL import Image\n","class RoadSignDataset(Dataset):\n","    def __init__(self, images_dir, labels_dir, transforms=None):\n","        self.images_dir = images_dir\n","        self.labels_dir = labels_dir\n","        self.transforms = transforms\n","        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg') or f.endswith('.png')])\n","\n","    def __len__(self):\n","        return len(self.image_files)\n","\n","    def __getitem__(self, idx):\n","        # Load image\n","        img_path = os.path.join(self.images_dir, self.image_files[idx])\n","        image = Image.open(img_path).convert(\"RGB\")\n","\n","        # Use the full image file name (excluding the extension) for the label file name\n","        label_file_name = os.path.splitext(self.image_files[idx])[0] + '.txt'\n","        label_path = os.path.join(self.labels_dir, label_file_name)\n","\n","        boxes = []\n","        labels = []\n","\n","        # Check if label file exists\n","        if os.path.exists(label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f:\n","                    data = line.strip().split()\n","                    class_id = int(data[0])\n","                    x_center, y_center, width, height = map(float, data[1:])\n","\n","                    # Convert YOLO format to (xmin, ymin, xmax, ymax) format\n","                    xmin = x_center - width / 2\n","                    ymin = y_center - height / 2\n","                    xmax = x_center + width / 2\n","                    ymax = y_center + height / 2\n","\n","                    boxes.append([xmin, ymin, xmax, ymax])\n","                    labels.append(class_id)\n","        else:\n","            print(f\"Warning: No label file found for {img_path}. Using an empty label.\")\n","\n","        # If no labels are found, set default empty tensors for boxes and labels\n","        if not boxes:\n","            boxes = torch.zeros((0, 4), dtype=torch.float32)\n","            labels = torch.zeros((0,), dtype=torch.int64)\n","\n","        # Convert to tensor\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","        image_id = torch.tensor([idx])\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n","\n","        target = {\n","            \"boxes\": boxes,\n","            \"labels\": labels,\n","            \"image_id\": image_id,\n","            \"area\": area,\n","            \"iscrowd\": iscrowd\n","        }\n","\n","        # Apply transforms\n","        if self.transforms:\n","            image = self.transforms(image)\n","\n","        return image, target\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:57:45.187769Z","iopub.status.busy":"2024-10-26T08:57:45.187396Z","iopub.status.idle":"2024-10-26T08:57:45.198879Z","shell.execute_reply":"2024-10-26T08:57:45.198022Z","shell.execute_reply.started":"2024-10-26T08:57:45.187725Z"},"trusted":true},"outputs":[],"source":["import torchvision.transforms as T\n","\n","transform = T.Compose([\n","    T.ToTensor(),\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:57:45.200402Z","iopub.status.busy":"2024-10-26T08:57:45.200032Z","iopub.status.idle":"2024-10-26T08:57:45.337944Z","shell.execute_reply":"2024-10-26T08:57:45.337129Z","shell.execute_reply.started":"2024-10-26T08:57:45.200360Z"},"trusted":true},"outputs":[],"source":["train_dataset = RoadSignDataset(\n","    images_dir='/kaggle/input/road-sign/Road_Sign_Detection  3/train/images/',\n","    labels_dir='/kaggle/input/road-sign/Road_Sign_Detection  3/train/labels/',\n","    transforms=train_transforms\n",")\n","\n","valid_dataset = RoadSignDataset(\n","    images_dir='/kaggle/input/road-sign/Road_Sign_Detection  3/valid/images/',\n","    labels_dir='/kaggle/input/road-sign/Road_Sign_Detection  3/valid/labels/',\n","    transforms=valid_transforms\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n","valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:57:45.339647Z","iopub.status.busy":"2024-10-26T08:57:45.339244Z","iopub.status.idle":"2024-10-26T08:57:45.343870Z","shell.execute_reply":"2024-10-26T08:57:45.342885Z","shell.execute_reply.started":"2024-10-26T08:57:45.339604Z"},"trusted":true},"outputs":[],"source":["# next(iter(train_loader))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T08:57:45.345205Z","iopub.status.busy":"2024-10-26T08:57:45.344925Z","iopub.status.idle":"2024-10-26T08:57:47.361670Z","shell.execute_reply":"2024-10-26T08:57:47.360815Z","shell.execute_reply.started":"2024-10-26T08:57:45.345166Z"},"trusted":true},"outputs":[],"source":["import torchvision\n","\n","# Set the number of classes based on data.yaml\n","num_classes = 25  \n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n","model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T09:47:45.820235Z","iopub.status.busy":"2024-10-26T09:47:45.819872Z","iopub.status.idle":"2024-10-26T17:52:18.689328Z","shell.execute_reply":"2024-10-26T17:52:18.687889Z","shell.execute_reply.started":"2024-10-26T09:47:45.820202Z"},"trusted":true},"outputs":[],"source":["import torch.optim as optim\n","from tqdm import tqdm\n","\n","# Define AdamW optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = optim.AdamW(params, lr=0.0001, weight_decay=0.0005)\n","\n","# Training loop\n","num_epochs = 50\n","save_path = \"fasterrcnn_resnet50_checkpoint.pth\"  # Define checkpoint path\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0  # Track training loss for the epoch\n","\n","    # Training phase\n","    for i, (images, targets) in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\")):\n","        images = [image.to(device) for image in images]\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        # Forward pass\n","        loss_dict = model(images, targets)\n","        losses = sum(loss for loss in loss_dict.values())\n","        train_loss += losses.item()\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","\n","        if i % 10 == 0:  # Print progress every 10 steps\n","            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i}/{len(train_loader)}], Loss: {losses.item()}\")\n","\n","    avg_train_loss = train_loss / len(train_loader)\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] Average Training Loss: {avg_train_loss:.4f}\")\n","\n","    # Validation phase\n","    model.train()  # Switch to train mode temporarily to get loss with targets\n","    val_loss = 0  # Track validation loss for the epoch\n","    with torch.no_grad():  # Ensure no gradients are calculated\n","        for images, targets in tqdm(valid_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n","            images = [image.to(device) for image in images]\n","            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","            # Forward pass for validation with targets to get loss\n","            loss_dict = model(images, targets)\n","            losses = sum(loss for loss in loss_dict.values())\n","            val_loss += losses.item()\n","\n","    avg_val_loss = val_loss / len(valid_loader)\n","    print(f\"Epoch [{epoch + 1}/{num_epochs}] Average Validation Loss: {avg_val_loss:.4f}\")\n","\n","    # Switch back to eval mode after validation\n","    model.eval()\n","\n","\n","    # Validation phase\n","#     model.eval()\n","#     val_loss = 0  # Track validation loss for the epoch\n","#     with torch.no_grad():  # No gradient calculation during validation\n","#         for images, targets in tqdm(valid_loader, desc=f\"Validation Epoch {epoch + 1}\"):\n","#             images = [image.to(device) for image in images]\n","#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","#             # Forward pass for validation\n","#             loss_dict = model(images, targets)\n","#             losses = sum(loss for loss in loss_dict.values())\n","#             val_loss += losses.item()\n","\n","#     avg_val_loss = val_loss / len(valid_loader)\n","#     print(f\"Epoch [{epoch + 1}/{num_epochs}] Average Validation Loss: {avg_val_loss:.4f}\")\n","\n","    # Save model checkpoint every 5 epochs\n","    if (epoch + 1) % 5 == 0:\n","        torch.save({\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'train_loss': avg_train_loss,\n","            'val_loss': avg_val_loss,\n","        }, save_path)\n","        print(f\"Checkpoint saved at epoch {epoch + 1}\")\n","\n","print(\"Training complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:07:16.671313Z","iopub.status.idle":"2024-10-26T09:07:16.671704Z","shell.execute_reply":"2024-10-26T09:07:16.671520Z","shell.execute_reply.started":"2024-10-26T09:07:16.671502Z"},"trusted":true},"outputs":[],"source":["# torch.save(model.state_dict(), \"fasterrcnn_resnet50.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T17:52:18.692176Z","iopub.status.busy":"2024-10-26T17:52:18.691519Z","iopub.status.idle":"2024-10-26T17:52:20.521140Z","shell.execute_reply":"2024-10-26T17:52:20.520199Z","shell.execute_reply.started":"2024-10-26T17:52:18.692129Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision\n","\n","# Number of classes (from data.yaml)\n","num_classes = 25  \n","\n","# Initialize the model and load the trained weights\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n","\n","# Load the model checkpoint\n","checkpoint_path = \"/kaggle/working/fasterrcnn_resnet50_checkpoint.pth\"  # Adjust path as necessary\n","checkpoint = torch.load(checkpoint_path, map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","model.to(device)\n","model.eval()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T17:52:20.522779Z","iopub.status.busy":"2024-10-26T17:52:20.522381Z","iopub.status.idle":"2024-10-26T17:52:20.643824Z","shell.execute_reply":"2024-10-26T17:52:20.643047Z","shell.execute_reply.started":"2024-10-26T17:52:20.522735Z"},"trusted":true},"outputs":[],"source":["from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","test_images_dir = '/kaggle/input/road-sign/Road_Sign_Detection  3/test'  # Adjust path as necessary\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Load test images\n","test_images = [f for f in os.listdir(test_images_dir) if f.endswith('.jpg') or f.endswith('.png')]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T17:52:20.645724Z","iopub.status.busy":"2024-10-26T17:52:20.645418Z","iopub.status.idle":"2024-10-26T17:52:38.528464Z","shell.execute_reply":"2024-10-26T17:52:38.527631Z","shell.execute_reply.started":"2024-10-26T17:52:20.645692Z"},"trusted":true},"outputs":[],"source":["predictions = []\n","\n","for img_name in test_images:\n","    img_path = os.path.join(test_images_dir, img_name)\n","    image = Image.open(img_path).convert(\"RGB\")\n","    image = test_transform(image).to(device)\n","    \n","    # Add a batch dimension\n","    with torch.no_grad():\n","        outputs = model([image])\n","    \n","    # Get the top predicted class for the first detected object, if available\n","    if len(outputs[0]['labels']) > 0:\n","        pred_class = outputs[0]['labels'][0].item()  # Get the first prediction label\n","    else:\n","        pred_class = None  # No prediction (optional: assign a default value)\n","    \n","    predictions.append({\"filename\": img_name, \"class\": pred_class})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T17:52:38.530717Z","iopub.status.busy":"2024-10-26T17:52:38.529754Z","iopub.status.idle":"2024-10-26T17:52:38.546647Z","shell.execute_reply":"2024-10-26T17:52:38.545740Z","shell.execute_reply.started":"2024-10-26T17:52:38.530659Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Create DataFrame for submission\n","submission_df = pd.DataFrame(predictions)\n","submission_df.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-26T18:26:12.343899Z","iopub.status.busy":"2024-10-26T18:26:12.343495Z","iopub.status.idle":"2024-10-26T18:26:12.362117Z","shell.execute_reply":"2024-10-26T18:26:12.361020Z","shell.execute_reply.started":"2024-10-26T18:26:12.343861Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Load submission.csv\n","submission_df = pd.read_csv(\"/kaggle/working/submission.csv\")\n","\n","# Define class names based on provided data.yaml\n","class_names = [\n","    '-Road narrows on right', 'Attention Please-', 'Beware of children', 'CYCLE ROUTE AHEAD WARNING', 'Crosswalk',\n","    'Dangerous Left Curve Ahead', 'Dangerous Right Curve Ahead', 'No Entry', 'No_Over_Taking', 'One way road',\n","    'Speed bump ahead', 'Speed limit', 'Speed limit 100 Kph', 'Speed limit 120 Kph', 'Speed limit 20 Kph',\n","    'Speed limit 30 Kph', 'Speed limit 40 Kph', 'Speed limit 50 Kph', 'Speed limit 60 Kph', 'Speed limit 70 Kph',\n","    'Speed limit 80 Kph', 'Speed limit 90 Kph', 'Stop_Sign', 'Uneven Road', 'roundabout'\n","]\n","\n","# Create a mapping from class numbers to names\n","class_mapping = {i: name for i, name in enumerate(class_names)}\n","\n","# Replace class numbers with class names\n","submission_df['class'] = submission_df['class'].apply(lambda x: class_mapping.get(x, 'none'))\n","\n","# Save the updated submission file\n","submission_df.to_csv(\"submission.csv\", index=False)\n","\n","print(\"Updated submission_with_names.csv created.\")\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5950776,"sourceId":9725290,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
