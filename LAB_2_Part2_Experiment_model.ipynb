{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e66d2ab-5072-4d1c-9b88-04f0cf709ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19251/720052994.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_padded = torch.load(f\"{data_path}train_tokenized_0_6_1950.pt\")\n",
      "/tmp/ipykernel_19251/720052994.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid_padded = torch.load(f\"{data_path}valid_tokenized_0_6_1950.pt\")\n",
      "/tmp/ipykernel_19251/720052994.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_padded = torch.load(f\"{data_path}test_tokenized_0_6_1950.pt\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "# Paths to saved files\n",
    "data_path = \"Data_Part2/\"\n",
    "glove_path = \"glove.6B.300d.txt\"\n",
    "\n",
    "# Load preprocessed data\n",
    "train_df = pd.read_csv(f\"{data_path}train_split_new.csv\")\n",
    "valid_df = pd.read_csv(f\"{data_path}valid_split_new.csv\")\n",
    "test_df = pd.read_csv(f\"{data_path}test_cleaned.csv\")\n",
    "\n",
    "# Load tokenized and padded data\n",
    "train_padded = torch.load(f\"{data_path}train_tokenized_0_6_1950.pt\")\n",
    "valid_padded = torch.load(f\"{data_path}valid_tokenized_0_6_1950.pt\")\n",
    "test_padded = torch.load(f\"{data_path}test_tokenized_0_6_1950.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c50da0e-e883-42b4-b7ad-5671d18deae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_path, vocab, embedding_dim=300):\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n",
    "    with open(glove_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = vector\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "# Load vocabulary\n",
    "vocab = {word: idx for idx, word in enumerate(train_df['text'].str.split().explode().unique())}\n",
    "\n",
    "# Load embeddings\n",
    "embedding_dim = 300\n",
    "glove_embeddings = load_glove_embeddings(glove_path, vocab, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "988a9df4-d2c1-41bd-b318-4562220e32ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText embeddings\n",
    "def load_fasttext_embeddings(fasttext_path, vocab, embedding_dim=300):\n",
    "    embeddings = np.random.uniform(-0.25, 0.25, (len(vocab), embedding_dim))\n",
    "    with open(fasttext_path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            if word in vocab:\n",
    "                embeddings[vocab[word]] = vector\n",
    "    return torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "# Load both embeddings\n",
    "fasttext_path = \"crawl-300d-2M.vec\"\n",
    "#glove_embeddings = load_glove_embeddings(glove_path, vocab, embedding_dim)\n",
    "fasttext_embeddings = load_fasttext_embeddings(fasttext_path, vocab, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd5740c-ad84-4792-9ab2-9ef9b62f5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target columns\n",
    "target_columns = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_df[target_columns].values, dtype=torch.float32)\n",
    "valid_labels = torch.tensor(valid_df[target_columns].values, dtype=torch.float32)\n",
    "\n",
    "# Ensure alignment\n",
    "assert train_padded.size(0) == len(train_df), \"Mismatch between train tokenized data and labels!\"\n",
    "assert valid_padded.size(0) == len(valid_df), \"Mismatch between valid tokenized data and labels!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ba04e-6d95-44f9-9420-46282cbb0a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b60fbefc-bac0-4cf6-ac15-4de648e4c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target columns\n",
    "target_columns = ['toxicity', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack', 'sexual_explicit']\n",
    "\n",
    "# Convert labels to tensors\n",
    "train_labels = torch.tensor(train_df[target_columns].values, dtype=torch.float32)\n",
    "valid_labels = torch.tensor(valid_df[target_columns].values, dtype=torch.float32)\n",
    "\n",
    "# Ensure alignment\n",
    "assert train_padded.size(0) == len(train_df), \"Mismatch between train tokenized data and labels!\"\n",
    "assert valid_padded.size(0) == len(valid_df), \"Mismatch between valid tokenized data and labels!\"\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_padded, train_labels)\n",
    "valid_dataset = TensorDataset(valid_padded, valid_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68b1d29b-6418-4d55-a763-cd97716194be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class LSTMOnly(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout, fasttext_embeddings):\n",
    "        super(LSTMOnly, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight.data.copy_(fasttext_embeddings)\n",
    "        self.embedding.weight.requires_grad = False  # Allow embeddings to be fine-tuned\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Final fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch_size, seq_len, hidden_dim * 2)\n",
    "        lstm_output = lstm_out[:, -1, :]  # Take the last hidden state for each sequence\n",
    "        lstm_output = self.dropout(lstm_output)\n",
    "        return torch.sigmoid(self.fc(lstm_output))\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "hidden_dim = 256\n",
    "output_dim = len(target_columns)\n",
    "dropout = 0.3\n",
    "model = LSTMAttention(len(vocab), embedding_dim, hidden_dim, output_dim, dropout, fasttext_embeddings)\n",
    "\n",
    "# Define loss, optimizer, and scheduler\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)  # Adding L2 regularization\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.7, patience=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05ae2df0-72db-4280-bdae-edb042500b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 462690\n",
      "Max index in train_padded: 462689\n",
      "Max index in valid_padded: 462668\n",
      "Max index in test_padded: 462664\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Max index in train_padded: {train_padded.max()}\")\n",
    "print(f\"Max index in valid_padded: {valid_padded.max()}\")\n",
    "print(f\"Max index in test_padded: {test_padded.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d16bf42a-4f40-4754-98ef-065c308c987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "train_padded[train_padded >= vocab_size] = 0\n",
    "valid_padded[valid_padded >= vocab_size] = 0\n",
    "test_padded[test_padded >= vocab_size] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0a0c88f-c956-4f95-9961-535f8d267f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 462690\n",
      "Max index in train_padded: 462689\n",
      "Max index in valid_padded: 462668\n",
      "Max index in test_padded: 462664\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Max index in train_padded: {train_padded.max()}\")\n",
    "print(f\"Max index in valid_padded: {valid_padded.max()}\")\n",
    "print(f\"Max index in test_padded: {test_padded.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b18719ac-ad6f-4317-90ee-5944eaa08c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9d1fec61-8b49-44a6-b776-b60113208e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout):\n",
    "        super(LSTMAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attention_layer = nn.Linear(hidden_dim * 2, 1)  # BiLSTM doubles the hidden dimension\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Final fully connected layer\n",
    "\n",
    "    def attention(self, lstm_outputs):\n",
    "        # Attention mechanism\n",
    "        weights = torch.tanh(self.attention_layer(lstm_outputs))  # Shape: (batch_size, seq_len, 1)\n",
    "        weights = torch.softmax(weights, dim=1)  # Shape: (batch_size, seq_len, 1)\n",
    "        weighted_output = torch.sum(weights * lstm_outputs, dim=1)  # Shape: (batch_size, hidden_dim * 2)\n",
    "        return weighted_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        lstm_out, _ = self.lstm(embedded)  # Shape: (batch_size, seq_len, hidden_dim * 2)\n",
    "        attended_output = self.attention(lstm_out)  # Apply Attention\n",
    "        attended_output = self.dropout(attended_output)\n",
    "        return torch.sigmoid(self.fc(attended_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d7d14-533a-4dc2-a961-02d7cceb3a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running configuration: Patience=3, LR=0.001, Batch Size=32, Dropout=0.4, Hidden Dim=128, Factor=0.4\n",
      "Epoch 1 Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  11%|█         | 4339/39482 [00:18<02:26, 240.28it/s]"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Define hyperparameter search space\n",
    "patience_values = [3, 4]\n",
    "learning_rates = [1e-3, 5e-4, 1e-4]\n",
    "batch_sizes = [32, 64]\n",
    "dropout_values = [0.4, 0.5, 0.6]\n",
    "hidden_dims = [128, 256]\n",
    "factors = [0.4, 0.5, 0.6]\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for patience, lr, batch_size, dropout, hidden_dim, factor in itertools.product(\n",
    "    patience_values, learning_rates, batch_sizes, dropout_values, hidden_dims, factors\n",
    "):\n",
    "    print(f\"Running configuration: Patience={patience}, LR={lr}, Batch Size={batch_size}, Dropout={dropout}, Hidden Dim={hidden_dim}, Factor={factor}\")\n",
    "\n",
    "    # Update DataLoader with new batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = LSTMAttention(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=300,\n",
    "    hidden_dim=hidden_dim,\n",
    "    output_dim=len(target_columns),\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load GloVe embeddings into the model\n",
    "    model.embedding.weight.data.copy_(glove_embeddings)\n",
    "    model.embedding.weight.requires_grad = False  # Freeze embeddings if necessary\n",
    "\n",
    "\n",
    "    # Define optimizer, scheduler, and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)  # Include L2 regularization\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    best_valid_loss = float('inf')\n",
    "    results = []\n",
    "    for epoch in range(5):  # Run for 5 epochs\n",
    "        print(f\"Epoch {epoch + 1} Training Begins\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(valid_loader, desc=f\"Validating Epoch {epoch + 1}\"):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        # Log losses\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        valid_loss_avg = valid_loss / len(valid_loader)\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss_avg:.4f}, Valid Loss = {valid_loss_avg:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if valid_loss_avg < best_valid_loss:\n",
    "            best_valid_loss = valid_loss_avg\n",
    "            model_save_path = f\"best_model_p{patience}_lr{lr}_bs{batch_size}_do{dropout}_hd{hidden_dim}_f{factor}.pth\"\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Model weights saved: {model_save_path}\")\n",
    "\n",
    "        # Record epoch results\n",
    "        results.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss_avg,\n",
    "            \"valid_loss\": valid_loss_avg,\n",
    "            \"best_valid_loss\": best_valid_loss\n",
    "        })\n",
    "\n",
    "        # Step scheduler\n",
    "        scheduler.step(valid_loss_avg)\n",
    "\n",
    "    # Save results for the current combination\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_filename = f\"results_p{patience}_lr{lr}_bs{batch_size}_do{dropout}_hd{hidden_dim}_f{factor}.csv\"\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "    print(f\"Results saved to {results_filename}\")\n",
    "\n",
    "    # Test loop and submission generation\n",
    "    test_dataset = TensorDataset(torch.tensor(test_padded, dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    test_predictions = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in tqdm(test_loader, desc=\"Making Predictions\"):\n",
    "            inputs = inputs[0].to(device)  # Extract Tensors from DataLoader and move to device\n",
    "            outputs = model(inputs)\n",
    "            test_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "    test_predictions = np.vstack(test_predictions)\n",
    "\n",
    "    # Threshold the predictions for multi-label classification\n",
    "    binary_predictions = (test_predictions > 0.6).astype(int)\n",
    "\n",
    "    # Create a DataFrame with the correct format\n",
    "    submission_filename = f\"submission_p{patience}_lr{lr}_bs{batch_size}_do{dropout}_hd{hidden_dim}_f{factor}.csv\"\n",
    "    submission = pd.DataFrame(binary_predictions, columns=target_columns)\n",
    "    submission.insert(0, 'id', test_df['id'])  # Ensure the 'id' column is included\n",
    "    submission.to_csv(submission_filename, index=False)\n",
    "    print(f\"Submission file created: {submission_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a496232-bcf4-4c48-a4bf-c63ebbe76083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 9871/9871 [05:13<00:00, 31.53it/s]\n",
      "Validating Epoch 1: 100%|██████████| 4231/4231 [01:05<00:00, 64.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.1102, Valid Loss = 0.1007\n",
      "Model weights saved for epoch 1.\n",
      "Epoch 2 Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 9871/9871 [05:28<00:00, 30.04it/s]\n",
      "Validating Epoch 2: 100%|██████████| 4231/4231 [01:04<00:00, 65.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.0996, Valid Loss = 0.0989\n",
      "Model weights saved for epoch 2.\n",
      "Epoch 3 Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 9871/9871 [05:28<00:00, 30.06it/s]\n",
      "Validating Epoch 3: 100%|██████████| 4231/4231 [01:03<00:00, 66.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.0981, Valid Loss = 0.0990\n",
      "Epoch 4 Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 9871/9871 [05:29<00:00, 29.92it/s]\n",
      "Validating Epoch 4: 100%|██████████| 4231/4231 [01:06<00:00, 63.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.0982, Valid Loss = 0.0992\n",
      "Epoch 5 Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 9871/9871 [05:35<00:00, 29.42it/s]\n",
      "Validating Epoch 5: 100%|██████████| 4231/4231 [01:04<00:00, 65.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.0981, Valid Loss = 0.0990\n",
      "Epoch 6 Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 9871/9871 [05:31<00:00, 29.80it/s]\n",
      "Validating Epoch 6: 100%|██████████| 4231/4231 [01:04<00:00, 65.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.0987, Valid Loss = 0.1006\n",
      "Epoch 7 Training Begins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7:   5%|▍         | 445/9871 [00:15<05:24, 29.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m     21\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop with explicit embedding control\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(10):  # Adjust the number of epochs as needed\n",
    "    print(f\"Epoch {epoch + 1} Training Begins\")\n",
    "    \n",
    "    # Set embeddings for the training phase\n",
    "    model.embedding.weight.data.copy_(glove_embeddings )  # Use FastText for training\n",
    "    model.embedding.weight.requires_grad = False  # Freeze embeddings during training\n",
    "\n",
    "    # Training Phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move inputs and targets to the correct device\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Set embeddings for the validation phase\n",
    "    model.embedding.weight.data.copy_(glove_embeddings )  # Use FastText for validation\n",
    "    model.embedding.weight.requires_grad = False  # Freeze embeddings during validation\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient calculation for validation\n",
    "        for inputs, targets in tqdm(valid_loader, desc=f\"Validating Epoch {epoch + 1}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            valid_loss += loss.item()\n",
    "    \n",
    "    # Calculate and Log Losses\n",
    "    train_loss_avg = train_loss / len(train_loader)\n",
    "    valid_loss_avg = valid_loss / len(valid_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss_avg:.4f}, Valid Loss = {valid_loss_avg:.4f}\")\n",
    "    \n",
    "    # Save Best Model\n",
    "    if valid_loss_avg < best_valid_loss:\n",
    "        best_valid_loss = valid_loss_avg\n",
    "        torch.save(model.state_dict(), f\"{data_path}best_model_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Model weights saved for epoch {epoch + 1}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a4ee864-0055-40f8-8057-7e6642a55e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19251/853097361.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_padded, dtype=torch.long))\n",
      "Making Predictions: 100%|██████████| 761/761 [00:09<00:00, 76.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary submission file created.\n"
     ]
    }
   ],
   "source": [
    "# Test loop and submission generation\n",
    "test_dataset = TensorDataset(torch.tensor(test_padded, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_predictions = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(test_loader, desc=\"Making Predictions\"):\n",
    "        inputs = inputs[0].to(device)  # Extract Tensors from DataLoader and move to device\n",
    "        outputs = model(inputs)\n",
    "        test_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "\n",
    "# Threshold the predictions for multi-label classification\n",
    "binary_predictions = (test_predictions > 0.6).astype(int)\n",
    "\n",
    "# Create a DataFrame with the correct format\n",
    "submission = pd.DataFrame(binary_predictions, columns=target_columns)\n",
    "submission.insert(0, 'id', test_df['id'])  # Ensure the 'id' column is included\n",
    "submission.to_csv(f\"{data_path}submission_LSTM.csv\", index=False)\n",
    "\n",
    "print(\"Binary submission file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bd1611e-4f2f-4bc7-b47b-d567f886c6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19251/3105052797.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "/tmp/ipykernel_19251/3105052797.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_dataset = TensorDataset(torch.tensor(test_padded, dtype=torch.long))\n",
      "Making Predictions: 100%|██████████| 1521/1521 [00:12<00:00, 121.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary submission file created.\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained model weights\n",
    "model_path = f\"{data_path}best_model_epoch_5.pth\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device)  # Ensure the model is on the correct device\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Test loop and submission generation\n",
    "test_dataset = TensorDataset(torch.tensor(test_padded, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(test_loader, desc=\"Making Predictions\"):\n",
    "        inputs = inputs[0].to(device)  # Extract Tensors from DataLoader and move to device\n",
    "        outputs = model(inputs)\n",
    "        test_predictions.append(outputs.cpu().numpy())\n",
    "\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "\n",
    "# Threshold the predictions for multi-label classification\n",
    "binary_predictions = (test_predictions > 0.6).astype(int)\n",
    "\n",
    "# Create a DataFrame with the correct format\n",
    "submission = pd.DataFrame(binary_predictions, columns=target_columns)\n",
    "submission.insert(0, 'id', test_df['id'])  # Ensure the 'id' column is included\n",
    "submission.to_csv(f\"{data_path}submission_LSTM.csv\", index=False)\n",
    "\n",
    "print(\"Binary submission file created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e95d3-8729-4136-86a7-0012938f8291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01fee9f-a616-4a7e-b388-0b71cb916d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b3a69-2081-45ca-be8c-b9d0f5a86c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6832947-56d8-423e-a033-64427e9b05a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27587292-99b0-4cfc-ad09-a10181488af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57a69b-bd8e-44ba-9306-087942cc7a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6618036-0364-4de5-aad1-dc52d19eb4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
